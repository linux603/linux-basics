

NFS (Network File System) 

what is NFS ??? 
=================
NFS is a distributed file system protocol that allows a Linux server to share directories over a network, 
so that other Linux machines can access those directories as if they were local filesystems.



Technical Definition :
=====================

NFS is a client–server based file sharing service where:

One system acts as an NFS server

Other systems act as NFS clients

Files are accessed over the network using TCP port 2049

Remote directories are mounted locally on client machines


Why NFS ???  

note:  its just for understanding purpose  (starting)
=========

Imagine a real application environment:

---> In real applications, users do NOT talk to one fixed server. They talk to a Load Balancer (LB).

=====

User Browser
     |
Load Balancer
     |
-----------------
|    |     |    |  
web1 web2 web3 web4  

---> Each request can go to a different backend web server
=====

 What Happens During File Uploads & App Operations (Without NFS) ??? 

 note:  Load balancer distributes requests, Each server has its own local disk, No shared filesystem

Important Rule:
Local disks are isolated — same path ≠ same data in each web server 


 Problem 1: Application Uploads Break

 What Happens

1. User uploads a file: data.txt
2. Load balancer sends upload request to web2
3. Application saves file to: web2:/appdata/uploads/data.txt
   
----> Critical Point :

 File is stored on web2’s local filesystem

 Other servers:
  web3:/appdata/uploads → EMPTY
  web4:/appdata/uploads → EMPTY
  
----> Next User Action

User refreshes page / clicks download / views profile.

This is a new HTTP request

Load balancer may send it to web3 or web4.

----> Result

Application looks for: /appdata/uploads/data.txt

But file does not exist on that server  (web3 or web4).

File not found
Broken user experience

----> Business Impact

 Users complain
 Upload feature unreliable
 Support tickets increase


Problem 2: Logs Are Split Across Servers

 ---> What Happens

Each server writes logs locally:

web2:/var/log/app.log
web3:/var/log/app.log
web4:/var/log/app.log

---> Why This Is a Problem

 A single user request may hit multiple servers
 Logs for one transaction are scattered
 No complete visibility


 ---> Impact

 Root cause analysis takes hours
 Production issues stay unresolved longer
 SLA violations


Problem 3: Application Binaries Drift

 ---> What Happens

Application deployed manually:

/opt/myapp

Versions across servers:

| Server | Version |
| ------ | ------- |
| web2   | v1.2    |
| web3   | v1.1    |
| web4   | v1.0    |


---> Why This Happens

 Manual deployments
 Missed updates
 Failed copy on one server


 ---> Result

 Random bugs
 Inconsistent behavior
 “Works on web2, fails on web4”

---> Very hard to debug


---> Business Impact

 Loss of trust
 Frequent rollbacks
 Deployment fear

 Problem 4: Manual Sync Is Not Scalable

 Tools Used

scp
rsync
cron jobs



 Hidden Problems

| Issue           | Explanation               |
| --------------- | ------------------------- |
| Race conditions | File accessed during copy |
| Partial sync    | Network breaks mid-copy   |
| Performance     | Heavy IO + network        |
| Human error     | Wrong path, wrong server  |


Works for 2 servers, fails for 10+ servers


 Problem 5: Scaling Becomes Painful

 Scenario: Add a New Server

web5

 ---> Required Manual Work

 Copy app binaries
 Sync uploads
 Fix permissions
 Verify data consistency
 Test manually


 ---> Why This Is Bad

 Scaling takes hours/days
 High chance of mistakes
 Delays business growth



The NFS Solution (What Changes)

            NFS Server (m1)
               |
           /appdata
               |
--------------------------------
|      |      |      |         |
m2     m3     m4     m5        m6

All app servers mount:  /appdata  → SAME DATA


Problem 1 Solved: Uploads Work Everywhere
Problem 2 Solved: Centralized Logs
Problem 3 Solved: Consistent Binaries
Problem 4 Solved: No Manual Sync
Problem 5 Solved: Easy Backup
Problem 6 Solved: Easy Scaling


note: to this its just for understanding purpose (ending )
=========


 NFS Concept :
============

| Term       | Meaning                              |
-----------------------------------------------------
| NFS Server | Machine that shares storage          |
| NFS Client | Machine that uses shared storage     |
| Export     | Directory shared by NFS server       |
| Mount      | Attaching remote directory locally   |

  NFS makes remote storage look like local storage


 DAS vs NAS
=============

Method 1: DAS (Direct Attached Storage)

Disk → m1 → NFS → Clients
✔ Disk physically attached to `m1`
✔ `m1` shares using NFS

Use case:
 Dev
 QA
 Small prod workloads


Method 2: NAS (NetApp / HP 3PAR)


NAS Storage → Network → Servers

✔ Storage independent of servers
✔ HA, Snapshots, Replication
✔ Enterprise grade

Use case:
 Banking
 E-commerce
 Critical prod systems


 NFS Internal Architecture :

1. Package = nfs-utils

Provides:
 binaries
 daemons
 client tools

2. NFS Services & Daemons

| Component  | Role                    |
----------------------------------------
| nfs-server | Main service            |
| nfsd       | Handles file read/write |
| mountd     | Mount requests          |
| lockd      | File locking            |
| statd      | Crash recovery          |

3. Network Port     2049/TCP

This port must be:
 open in firewall
 allowed in cloud NSG
 reachable from client


NFS Server Configuration (m1) — STEP BY STEP
==============================================


 STEP 1️⃣ Login & Root Access
    ssh m1
    sudo -i
 STEP 2️⃣ Install NFS Package
    yum install -y nfs-utils
    Verify:
    rpm -qa | grep nfs

 STEP 3️⃣ Start NFS Service
    systemctl enable --now nfs-server
    systemctl status nfs-server

    Expected:
    Active: active (running)

 STEP 4️⃣ Prepare Directory to Share
    mkdir /appdata
    Mount disk:
    mount /dev/sdb/app-lv1 /appdata
    Verify:
    df -h | grep appdata

 STEP 5️⃣ Export Configuration (`/etc/exports`)
    This is the HEART of NFS
    Format:
    <directory> <client> (options)

    Example 1️⃣: Single Client
            /appdata 192.168.219.10(rw,sync)

    Example 2️⃣: Multiple Clients
            /appdata 192.168.219.10(rw,sync)
            /appdata 192.168.219.11(rw,sync)

    Example 3️⃣: Subnet Based (Most Common)
            /appdata 192.168.219.0/24(rw,sync)

note:
====
| Option           | Meaning                     |
--------------------------------------------------
| `rw`             | Read + Write                |
| `ro`             | Read only                   |
| `sync`           | Write to disk before reply  |
| `async`          | Faster but risky            |
| `no_root_squash` | Allow root full access      |
| `root_squash`    | Map root → nobody (default) |

    STEP 6️⃣ Apply Export Rules

        exportfs -rav

            Meaning:
            `r` → re-export
            `a` → all
            `v` → verbose

        Verify:

        exportfs -v

 STEP 7️⃣ Firewall Configuration

        firewall-cmd --permanent --add-service=nfs
        firewall-cmd --reload

        Verify:
        firewall-cmd --list-services




 STEP 8️⃣ SELinux Configuration (MOST MISSED STEP)

        getenforce

        Allow NFS:
        setsebool -P nfs_export_all_rw 1


 STEP 9️⃣ Cloud / NSG Check

✔ Allow 2049/TCP
✔ Source = client subnet


 NFS Client Configuration (m2/m3)
================================


 STEP 1️⃣ Install Client Tools

    yum install -y nfs-utils
    Client does NOT need `nfs-server` service


 STEP 2️⃣ Verify Server Export

    showmount -e 192.168.219.134

    Output:
    Export list for 192.168.219.134:
    /appdata 192.168.219.0/24




 STEP 3️⃣ Create Local Mount Point

        mkdir /appdata

 STEP 4️⃣ Mount NFS Share

    mount -t nfs 192.168.219.134:/appdata /appdata

     df -h | grep appdata


 STEP 5️⃣ Test Read & Write

        touch /appdata/testfile
        ls -l /appdata


 STEP 6️⃣ Persistent Mount (fstab)


    vi /etc/fstab

    Add:
    192.168.219.134:/appdata /appdata nfs defaults,_netdev 0 0

    Test:
    mount -a
